{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|               tweet|label|                text|              tokens|       no-stop-words|              ngrams|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|bunu da gördük uz...|    0|bunu da gördük uz...|[bunu, da, gördük...|[bunu, gördük, uz...|[bunu gördük, gör...|\n",
      "|filmi begenmeyenl...|    0|filmi begenmeyenl...|[filmi, begenmeye...|[filmi, begenmeye...|[filmi begenmeyen...|\n",
      "|günde 1 film izle...|    0|günde 1 film izle...|[günde, 1, film, ...|[günde, 1, film, ...|[günde 1, 1 film,...|\n",
      "|afisine bakip ald...|    0|afisine bakip ald...|[afisine, bakip, ...|[afisine, bakip, ...|[afisine bakip, b...|\n",
      "|sadece insanlarin...|    0|sadece insanlarin...|[sadece, insanlar...|[sadece, insanlar...|[sadece insanlari...|\n",
      "|ucuz bir aksiyon ...|    0|ucuz bir aksiyon ...|[ucuz, bir, aksiy...|[ucuz, bir, aksiy...|[ucuz bir, bir ak...|\n",
      "|olmamis diyor pua...|    0|olmamis diyor pua...|[olmamis, diyor, ...|[olmamis, diyor, ...|[olmamis diyor, d...|\n",
      "|kesinlikle çok kö...|    0|kesinlikle çok kö...|[kesinlikle, çok,...|[kesinlikle, kötü...|[kesinlikle kötüy...|\n",
      "|ben nasil bir fil...|    0|ben nasil bir fil...|[ben, nasil, bir,...|[ben, nasil, bir,...|[ben nasil, nasil...|\n",
      "|bu yüzler fazla t...|    0|bu yüzler fazla t...|[bu, yüzler, fazl...|[yüzler, fazla, t...|[yüzler fazla, fa...|\n",
      "|. çok ilginç yaa ...|    0|  çok ilginç yaa ...|[, , çok, ilginç,...|[, , ilginç, yaa,...|[ ,  ilginç, ilgi...|\n",
      "|bence bu film hiç...|    0|bence bu film hiç...|[bence, bu, film,...|[bence, film, güz...|[bence film, film...|\n",
      "|valla ben begenme...|    0|valla ben begenme...|[valla, ben, bege...|[valla, ben, bege...|[valla ben, ben b...|\n",
      "|gönül ister milyo...|    0|gönül ister milyo...|[gönül, ister, mi...|[gönül, ister, mi...|[gönül ister, ist...|\n",
      "|dün aksam bu film...|    0|dün aksam bu film...|[dün, aksam, bu, ...|[dün, aksam, film...|[dün aksam, aksam...|\n",
      "|cok kötü olmus......|    0|çok kötü olmus   ...|[çok, kötü, olmus...|[kötü, olmus, , ,...|[kötü olmus, olmu...|\n",
      "|igrenç yaaa.acaba...|    0|igrenç yaaa acaba...|[igrenç, yaaa, ac...|[igrenç, yaaa, fi...|[igrenç yaaa, yaa...|\n",
      "|saçma sapan bir f...|    0|saçma sapan bir f...|[saçma, sapan, bi...|[saçma, sapan, bi...|[saçma sapan, sap...|\n",
      "|çok güzel komedi ...|    0|çok güzel komedi ...|[çok, güzel, kome...|[güzel, komedi, f...|[güzel komedi, ko...|\n",
      "|beklentimi yüksek...|    0|beklentimi yüksek...|[beklentimi, yüks...|[beklentimi, yüks...|[beklentimi yükse...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|               tweet|label|                text|              tokens|       no-stop-words|              ngrams|            features|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|bunu da gördük uz...|    0|bunu da gördük uz...|[bunu, da, gördük...|[bunu, gördük, uz...|[bunu gördük, gör...|[0.0,0.0,0.0,0.0,...|\n",
      "|filmi begenmeyenl...|    0|filmi begenmeyenl...|[filmi, begenmeye...|[filmi, begenmeye...|[filmi begenmeyen...|[0.00357536678867...|\n",
      "|günde 1 film izle...|    0|günde 1 film izle...|[günde, 1, film, ...|[günde, 1, film, ...|[günde 1, 1 film,...|[0.00152859701120...|\n",
      "|afisine bakip ald...|    0|afisine bakip ald...|[afisine, bakip, ...|[afisine, bakip, ...|[afisine bakip, b...|[0.0,0.0,0.0,0.0,...|\n",
      "|sadece insanlarin...|    0|sadece insanlarin...|[sadece, insanlar...|[sadece, insanlar...|[sadece insanlari...|[0.00177872333336...|\n",
      "|ucuz bir aksiyon ...|    0|ucuz bir aksiyon ...|[ucuz, bir, aksiy...|[ucuz, bir, aksiy...|[ucuz bir, bir ak...|[3.31736125706418...|\n",
      "|olmamis diyor pua...|    0|olmamis diyor pua...|[olmamis, diyor, ...|[olmamis, diyor, ...|[olmamis diyor, d...|[0.0,0.0,0.0,0.0,...|\n",
      "|kesinlikle çok kö...|    0|kesinlikle çok kö...|[kesinlikle, çok,...|[kesinlikle, kötü...|[kesinlikle kötüy...|[1.06595591792175...|\n",
      "|ben nasil bir fil...|    0|ben nasil bir fil...|[ben, nasil, bir,...|[ben, nasil, bir,...|[ben nasil, nasil...|[9.44662839174270...|\n",
      "|bu yüzler fazla t...|    0|bu yüzler fazla t...|[bu, yüzler, fazl...|[yüzler, fazla, t...|[yüzler fazla, fa...|[-0.0036363180709...|\n",
      "|. çok ilginç yaa ...|    0|  çok ilginç yaa ...|[, , çok, ilginç,...|[, , ilginç, yaa,...|[ ,  ilginç, ilgi...|[-9.4607884616211...|\n",
      "|bence bu film hiç...|    0|bence bu film hiç...|[bence, bu, film,...|[bence, film, güz...|[bence film, film...|[4.11890994291752...|\n",
      "|valla ben begenme...|    0|valla ben begenme...|[valla, ben, bege...|[valla, ben, bege...|[valla ben, ben b...|[2.16762069612741...|\n",
      "|gönül ister milyo...|    0|gönül ister milyo...|[gönül, ister, mi...|[gönül, ister, mi...|[gönül ister, ist...|[-0.0018142773346...|\n",
      "|dün aksam bu film...|    0|dün aksam bu film...|[dün, aksam, bu, ...|[dün, aksam, film...|[dün aksam, aksam...|[7.58060438480849...|\n",
      "|cok kötü olmus......|    0|çok kötü olmus   ...|[çok, kötü, olmus...|[kötü, olmus, , ,...|[kötü olmus, olmu...|[-0.0017776274513...|\n",
      "|igrenç yaaa.acaba...|    0|igrenç yaaa acaba...|[igrenç, yaaa, ac...|[igrenç, yaaa, fi...|[igrenç yaaa, yaa...|[0.0,0.0,0.0,0.0,...|\n",
      "|saçma sapan bir f...|    0|saçma sapan bir f...|[saçma, sapan, bi...|[saçma, sapan, bi...|[saçma sapan, sap...|[-0.0019484861521...|\n",
      "|çok güzel komedi ...|    0|çok güzel komedi ...|[çok, güzel, kome...|[güzel, komedi, f...|[güzel komedi, ko...|[0.00241326500492...|\n",
      "|beklentimi yüksek...|    0|beklentimi yüksek...|[beklentimi, yüks...|[beklentimi, yüks...|[beklentimi yükse...|[2.85079639235680...|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "Test Accuracy :  0.6046511627906976\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.ml.feature import Word2Vec,Tokenizer,StopWordsRemover,NGram\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkML Depression Analysis\").getOrCreate()\n",
    "dataDF= spark.read.option(\"inferSchema\",\"True\")\\\n",
    "                        .option(\"delimiter\",\"\\t\")\\\n",
    "                        .csv(\"datasets/movie_turkish_train.txt\")\n",
    "\n",
    "dataDF = dataDF.withColumnRenamed(\"_c0\",\"tweet\")\n",
    "dataDF = dataDF.withColumnRenamed(\"_c1\",\"label\")\n",
    "\n",
    "\n",
    "def removeCharacters(text):\n",
    "    text = text.replace(\",\",\" \")\n",
    "    text = text.replace(\".\",\" \")\n",
    "    text = text.replace(\"!\",\" \")\n",
    "    text = text.replace(\"?\",\" \")\n",
    "    text = text.replace(\"cok\",\"çok\")\n",
    "    return text\n",
    "    \n",
    "removeUDF = UserDefinedFunction(removeCharacters,StringType())\n",
    "dataDF = dataDF.withColumn(\"text\",removeUDF('tweet'))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='text',outputCol='tokens')\n",
    "dataDF = tokenizer.transform(dataDF)\n",
    "\n",
    "\n",
    "remover = StopWordsRemover(inputCol='tokens',outputCol='no-stop-words',stopWords=StopWordsRemover.loadDefaultStopWords('turkish'))\n",
    "dataDF = remover.transform(dataDF)\n",
    "\n",
    "\n",
    "ng = NGram(n=2,inputCol=\"no-stop-words\",outputCol=\"ngrams\")\n",
    "dataDF = ng.transform(dataDF)\n",
    "\n",
    "dataDF.show()\n",
    "\n",
    "\n",
    "vSize = 400\n",
    "w2v = Word2Vec(inputCol='ngrams',outputCol='features',vectorSize=vSize,windowSize=23,minCount=5)\n",
    "dataDF = w2v.fit(dataDF).transform(dataDF)\n",
    "\n",
    "dataDF.show()\n",
    "\n",
    "\n",
    "dataDF = dataDF.select('features','label')\n",
    "dataDF.printSchema()\n",
    "\n",
    "trainDF,testDF = dataDF.randomSplit([0.75,0.25],seed=123)\n",
    "\n",
    "\n",
    "mlpClassifier = MultilayerPerceptronClassifier(layers=[vSize,2,2])\n",
    "model = mlpClassifier.fit(trainDF)\n",
    "\n",
    "\n",
    "resultDF = model.transform(testDF) #Prediction\n",
    "#resultDF.show(100)\n",
    "\n",
    "eva = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = eva.evaluate(resultDF)\n",
    "print(\"Test Accuracy : \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
